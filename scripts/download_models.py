# scripts/download_models.py
#!/usr/bin/env python3
"""
ä¸‹è¼‰å’Œè¨­ç½® AI æ¨¡å‹
"""
import os
from pathlib import Path
from huggingface_hub import snapshot_download
import argparse


def download_model(model_name, cache_dir="./models", force_download=False):
    """Download model from Hugging Face"""
    try:
        print(f"ğŸ“¥ ä¸‹è¼‰æ¨¡å‹: {model_name}")

        local_path = snapshot_download(
            repo_id=model_name,
            cache_dir=cache_dir,
            force_download=force_download,
            resume_download=True,
        )

        print(f"âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ: {local_path}")
        return local_path

    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è¼‰å¤±æ•—: {e}")
        return None


def main():
    parser = argparse.ArgumentParser(description="ä¸‹è¼‰ VisionQuest æ¨¡å‹")
    parser.add_argument("--model", help="æŒ‡å®šæ¨¡å‹åç¨±")
    parser.add_argument("--all", action="store_true", help="ä¸‹è¼‰æ‰€æœ‰é è¨­æ¨¡å‹")
    parser.add_argument("--cache-dir", default="./models", help="æ¨¡å‹å¿«å–ç›®éŒ„")
    parser.add_argument("--force", action="store_true", help="å¼·åˆ¶é‡æ–°ä¸‹è¼‰")

    args = parser.parse_args()

    # Default models for Phase 2
    default_models = {
        "caption": "Salesforce/blip2-opt-2.7b",
        "vqa": "llava-hf/llava-v1.6-mistral-7b-hf",
        "llm": "Qwen/Qwen2-7B-Instruct",
        "embeddings": "BAAI/bge-m3",
    }

    if args.model:
        download_model(args.model, args.cache_dir, args.force)
    elif args.all:
        print("ğŸ“¦ ä¸‹è¼‰æ‰€æœ‰é è¨­æ¨¡å‹...")
        for model_type, model_name in default_models.items():
            print(f"\nğŸ”„ ä¸‹è¼‰ {model_type} æ¨¡å‹...")
            download_model(model_name, args.cache_dir, args.force)
    else:
        print("å¯ç”¨çš„é è¨­æ¨¡å‹:")
        for model_type, model_name in default_models.items():
            print(f"  {model_type}: {model_name}")
        print("\nä½¿ç”¨ --all ä¸‹è¼‰æ‰€æœ‰æ¨¡å‹ï¼Œæˆ– --model <æ¨¡å‹åç¨±> ä¸‹è¼‰ç‰¹å®šæ¨¡å‹")


if __name__ == "__main__":
    main()
